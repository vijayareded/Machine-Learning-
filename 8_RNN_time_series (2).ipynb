{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ18d98EpsSd"
      },
      "source": [
        "# Using Recurrent Neural Networks (RNN) for time series forecasting\n",
        "\n",
        "In this activity we will use Recurernt Neural Networks (RNN) for time series foreacasting. \n",
        "Remember that RNNs are suitable when we want to model data with sequential or temporal structures. \n",
        "\n",
        "A \"time series\" is generally the observation of a variable over time, see https://en.wikipedia.org/wiki/Time_series for more details. \n",
        "Time series forecasting is about making predictions about the future based on past data. \n",
        "We will work on a relatively simple example, trying to predict Google stock prices (univariate time series), without getting into details on the statistics and math behind it. \n",
        "\n",
        "For an overview of non-DeepLearning models for this task see [here](https://towardsdatascience.com/the-complete-guide-to-time-series-analysis-and-forecasting-70d476bfe775) \n",
        "\n",
        "Note that for rigorous time series analysis we would need other analysis than what is presented this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0kotE548aTr"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "Upload the \"Google_test.csv\" and \"Google_train.csv\" datasets to your session before proceeding (both are posted on elearn). \n",
        "\n",
        "You can easily download similar datasets as csv files for any Stock ticker from https://finance.yahoo.com/. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4Cg7d-NcxIv"
      },
      "source": [
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyoFKCK2XtV3"
      },
      "source": [
        "### Training data\n",
        "Our training data is daily observations of Google stock price from *January 2010* till *January 2016*; 1529 observations over time\n",
        "\n",
        "We will use only the **Opening** price values. Observations of a single variable over time is also called a univariate time series.\n",
        "We try to predict future Opening prices based on its past values. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wtfcY6bw7Fc"
      },
      "source": [
        "dataset_train = pd.read_csv('Google_train.csv')\n",
        "training_set = dataset_train.iloc[:, 1:2].values # use only Opening price as input data\n",
        "dataset_train.head(2), dataset_train.tail(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9QVQj5urdwT"
      },
      "source": [
        "training_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE_XDFQ5jY14"
      },
      "source": [
        "### OPTIONAL alternative training data\n",
        "Alternatively, you can use the \"yfinance\" library to directly pull stock data from yahoo finance using a company's ticker. \n",
        "Uncomment the code and use it instead of the provided datasets.\n",
        "\n",
        "If you use this section, also make sure to pull corresponding testing data (see **OPTIONAL alternative testing data** in TOC). \n",
        "\n",
        "As is, it pulls the same dates for Google stock as in the provided dataset. You can experiment using other timeframes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YlDxS4ZkKfV"
      },
      "source": [
        "!pip install yfinance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oqz4NW7UjIk0"
      },
      "source": [
        "import yfinance as yf\n",
        "dataset_train=yf.download(\"GOOGL\", start=\"2010-01-01\", end=\"2016-01-30\")\n",
        "training_set=dataset_train[['Adj Close']].values #using adjusted closing prices\n",
        "training_set.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yctyTXOj-Fi"
      },
      "source": [
        "### Scaling our data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vz-XUdl6yCBd"
      },
      "source": [
        "# As we now, we need to scale our dataet\n",
        "# Feature Scaling using MinMax scaling\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "sc = MinMaxScaler(feature_range = (0, 1))\n",
        "training_set_scaled = sc.fit_transform(training_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMa8c7_X1JYq"
      },
      "source": [
        "len(training_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_ui5k3TZTW7"
      },
      "source": [
        "### Specifying time window (time steps)\n",
        "We will look at the time window of 60 past days (60 time steps) to make predictions for today.\n",
        "Our model will learn the current value based on the past window. \n",
        "\n",
        "This value can be tuned to potentially improve model performance depending on the available data. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_OjqJ3gai-9"
      },
      "source": [
        "# Creating a data structure with 60 timesteps and 1 output\n",
        "window=60 #change this value to try another window size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX1ITj54yQUd"
      },
      "source": [
        "\n",
        "# i.e., at each time t we look at the 60 time steps before, t-1, ...,t-60 (3 previous financial months)\n",
        "X_train = [] #each observation contains the 60 prices on the 60 time points before t\n",
        "y_train = [] #each observation is the stock price on time t\n",
        "for i in range(window, len(training_set)): #(60,1529)\n",
        "    X_train.append(training_set_scaled[i-window:i, 0])\n",
        "    y_train.append(training_set_scaled[i, 0])\n",
        "    \n",
        "X_train, y_train = np.array(X_train), np.array(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5A-eQmlaFNw"
      },
      "source": [
        "Our data has to be reshaped into a tensor (3d data structure) before we can feed it to a RNN. \n",
        "The tensor that can be feed to a RNN has the following structure:\n",
        "* number of observations = length of training data - window =1529-60\n",
        "* number of time steps; size of time window \n",
        "* number of variables/features = 1 for univariate time series "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZQU0zSTyYLk"
      },
      "source": [
        "# Reshaping the data so that it fits the format required for the RNN input layer\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "# 3D data structure (tensor) allowing for more than 1 variable, (number of observations,number of time steps,1)\n",
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5l9IMn02BOLz"
      },
      "source": [
        "### Testing Data\n",
        "Our test data is daily observations of Google stock price from *February 2016* till *April 2016*; 42 observations.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J20ts_CycYqV"
      },
      "source": [
        "dataset_test = pd.read_csv('Google_test.csv')\n",
        "dataset_test.head(2), dataset_test.tail(2), len(dataset_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmTc3njOsoZM"
      },
      "source": [
        "### OPTIONAL alternative testing data\n",
        "Use this to pull testing data if you have pulled your training data using the alternative option.\n",
        "\n",
        "As is, it pull the same dates for Google as in the provided dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnQP4Ll0rwB9"
      },
      "source": [
        "#import yfinance as yf\n",
        "#dataset_test=yf.download(\"GOOGL\", start=\"2016-02-01\", end=\"2016-04-01\")\n",
        "#training_set=dataset_test[['Adj Close']].values #using adjusted closing prices\n",
        "#training_set.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erlBqL2VdKfn"
      },
      "source": [
        "we process our test data similar to what we did for the training data above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gw8wzIpjxXHN"
      },
      "source": [
        "# window=60\n",
        "dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0)\n",
        "inputs = dataset_total[len(dataset_total) - len(dataset_test) - window:].values  \n",
        "inputs = inputs.reshape(-1,1)\n",
        "inputs = sc.transform(inputs) \n",
        "\n",
        "X_test = [] #each observation contains the 60 prices on the 60 time points before t\n",
        "y_test = [] #each observation is the stock price on time t\n",
        "for i in range(window, len(inputs)): \n",
        "    X_test.append(inputs[i-window:i, 0])\n",
        "    y_test.append(inputs[i, 0])\n",
        "    \n",
        "X_test, y_test = np.array(X_test), np.array(y_test)\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8pCMJ203mce"
      },
      "source": [
        "#let's make sure our datasets are shaped correctly before we proceed...\n",
        "print(X_train.shape, y_train.shape,X_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1h0rxkoN8XIF"
      },
      "source": [
        "## Building the first RNN model with LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wp7JDBBmzKBz"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ldi9QRuhvPl"
      },
      "source": [
        "We will build a network with \n",
        "* an input layer \n",
        "* two LSTM layers (each with 50 units) \n",
        "* each followed by a dropout layer to precent overfitting on the training data (see notebook 7 for more details on Dropout). \n",
        "* our ouput layer has 1 node\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJRXc9MJznFk"
      },
      "source": [
        "# Initialisi#ng the RNN\n",
        "ts= keras.Sequential()\n",
        "\n",
        "# Adding the first LSTM layer and some Dropout regularisation\n",
        "# Notice we specify the input layer here as well using input_shape=(60,1)\n",
        "ts.add(layers.GRU(units = 64, return_sequences = True, input_shape =(60,1) ))#(X_train.shape[1], X_train.shape[2])))\n",
        "#ts.add(layers.Dropout(0.2))  #randomly drops 20% of observations to avoid overfitting\n",
        "\n",
        "# Adding a second LSTM layer and some Dropout regularisation\n",
        "# Note that the \"return_sequences=False (default value)\" for the last RNN layer and True for previous layers\n",
        "ts.add(layers.GRU(units = 32)) \n",
        "#ts.add(layers.Dropout(0.2))\n",
        "\n",
        "# Adding the output layer\n",
        "ts.add(layers.Dense(units = 1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wB5RaCRG5ne0"
      },
      "source": [
        "ts.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfST4MnK5zKq"
      },
      "source": [
        "# we use \"Mean Squared Error\" as our loss function \n",
        "ts.compile(optimizer = 'adam', #can also try  optimizer='rmsprop'\n",
        "           loss = 'mean_squared_error', # mean_absolute_error\n",
        "           metrics=['MeanSquaredError', 'MeanAbsoluteError', 'MeanAbsolutePercentageError']) #MSE, MAE, MAPE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kECrmlzS51JY"
      },
      "source": [
        "# Fitting the RNN to the Training set\n",
        "history_LSTM=ts.fit(X_train, y_train, \n",
        "               epochs = 10, batch_size = 30,verbose=1,\n",
        "               validation_data=(X_test,y_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaLofvKP8Q13"
      },
      "source": [
        "### Evaluating the LSTM network and making predictions\n",
        "Visualizing the loss (MSE) as the model trains over several epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d85BHyq-zSJd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history_LSTM.history['loss'],label=\"training\")\n",
        "plt.plot(history_LSTM.history['val_loss'],label=\"validation\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWe6PR5-tL7f"
      },
      "source": [
        "### Question 1\n",
        "Would improving the number of epochs (for training the model) improve the model fit? How do you know?\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LyiBjk5uUNC"
      },
      "source": [
        "Let's predict the future stock prices and see how it compares with the actual prices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JC-6E-RpzDnC"
      },
      "source": [
        "real_stock_price = dataset_test.iloc[:, 1:2].values #outcome var=open price\n",
        "len(real_stock_price)\n",
        "#this is the number of future time units we will make predictions for"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS1Lt17yT0nj"
      },
      "source": [
        "predicted_stock_price = ts.predict(X_test)\n",
        "# note that we need to inverse_transform the predicted values \n",
        "# because the predictions are on the scaled [0,1] range (we MinMax scaled our data before training)\n",
        "predicted_stock_price_LSTM = sc.inverse_transform(predicted_stock_price)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riiaaOQH9qWt"
      },
      "source": [
        "# Visualising the results\n",
        "plt.plot(real_stock_price, color = 'red', label = 'Real Stock Price')\n",
        "plt.plot(predicted_stock_price_LSTM, color = 'blue', label = 'Predicted Stock Price')\n",
        "plt.title('Google Stock Price Prediction')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Google Stock Price')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEVwW4S1u6Uo"
      },
      "source": [
        "### RMSE, MAE metrics for LSTM network \n",
        "RMSE are MAE are both reasonable metrics to compare performance of forecasting models for time series (given the same data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aqzN3rhChXa"
      },
      "source": [
        "from sklearn import metrics\n",
        "import math\n",
        "#print('Coefficients: \\n', lin_reg.coef_) #regression coefficients\n",
        "print(\"Root Mean squared error: %.2f\" % math.sqrt(metrics.mean_squared_error(real_stock_price, predicted_stock_price_LSTM))) #RMSE\n",
        "print(\"Mean absolute error: %.2f\" % metrics.mean_absolute_error(real_stock_price, predicted_stock_price_LSTM)) #MAE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWazgDl-iABY"
      },
      "source": [
        "## Building another RNN model with GRUs\n",
        "\n",
        "Let's build the same network using GRU cells instead of LSTM. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3nHZJIniWfO"
      },
      "source": [
        "# Initialisi#ng the RNN\n",
        "ts2= keras.Sequential()\n",
        "\n",
        "# Adding the first GRU layer and some Dropout regularisation\n",
        "# Notice we specify the input layer here as well using input_shape=(60,1)\n",
        "ts2.add(layers.GRU(units = 50, return_sequences = True, input_shape = (X_train.shape[1], X_train.shape[2])))\n",
        "ts2.add(layers.Dropout(0.2))  #randomly drops 20% of observations to avoid overfitting\n",
        "\n",
        "# Adding a second GRU layer and some Dropout regularisation\n",
        "# Note that the \"return_sequences=False (default value)\" for the last RNN layer and True for previous layers\n",
        "ts2.add(layers.GRU(units = 50)) \n",
        "ts2.add(layers.Dropout(0.2))\n",
        "\n",
        "# Adding the output layer\n",
        "ts2.add(layers.Dense(units = 1))\n",
        "\n",
        "ts2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAZT9NZQv8Xg"
      },
      "source": [
        "# we use \"Mean Squared Error\" as our loss function \n",
        "ts2.compile(optimizer = 'adam', #can also try  optimizer='rmsprop'\n",
        "           loss = 'mean_squared_error', # can also try mean_absolute_error  \n",
        "           metrics=['MeanSquaredError', 'MeanAbsoluteError', 'MeanAbsolutePercentageError']) #MSE, MAE, MAPE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoCh39UQv_Dc"
      },
      "source": [
        "# Fitting the RNN to the Training set\n",
        "history_GRU=ts2.fit(X_train, y_train, \n",
        "               epochs = 10, batch_size = 30,verbose=1,\n",
        "               validation_data=(X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIex5gsMiWXr"
      },
      "source": [
        "### Evaluating the GRU network and making predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7cVW6i9wr3Y"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history_GRU.history['loss'],label=\"training\")\n",
        "plt.plot(history_GRU.history['val_loss'],label=\"validation\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iob4OcXHywPG"
      },
      "source": [
        "###Question 2 \n",
        "Is this network overfitting the training data? How do you know?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7comd6_oxVFj"
      },
      "source": [
        "predicted_stock_price = ts2.predict(X_test)\n",
        "# note that we need to inverse_transform the predicted values \n",
        "# because the predictions are on the scaled [0,1] range (we MinMax scaled our data before training)\n",
        "predicted_stock_price_GRU = sc.inverse_transform(predicted_stock_price)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7FNrltkxYlC"
      },
      "source": [
        "# Visualising the results\n",
        "plt.plot(real_stock_price, color = 'red', label = 'Real Stock Price')\n",
        "plt.plot(predicted_stock_price_GRU, color = 'blue', label = 'Predicted Stock Price (GRU)')\n",
        "plt.title('Google Stock Price Prediction')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Google Stock Price')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybv8TlsBwkV-"
      },
      "source": [
        "### RMSE, MAE metrics for GRU network "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVFqU_IQx5yW"
      },
      "source": [
        "print(\"Root Mean squared error: %.2f\" % math.sqrt(metrics.mean_squared_error(real_stock_price, predicted_stock_price_GRU))) #RMSE\n",
        "print(\"Mean absolute error: %.2f\" % metrics.mean_absolute_error(real_stock_price, predicted_stock_price_GRU)) #MAE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBe93kuPybZC"
      },
      "source": [
        "##Comparing the LSTM and GRU models \n",
        "Let's plot both models' predictions and the actual values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ty6-yAfJygPS"
      },
      "source": [
        "plt.plot(real_stock_price, color = 'red', label = 'Real Stock Price')\n",
        "plt.plot(predicted_stock_price_GRU, color = 'blue', label = 'Predicted Stock Price (GRU)')\n",
        "plt.plot(predicted_stock_price_LSTM, color = 'green', label = 'Predicted Stock Price (LSTM)')\n",
        "plt.title('Google Stock Price Prediction')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Google Stock Price')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuzbfZeQy7yg"
      },
      "source": [
        "### Question 3\n",
        "Based on the above plot, which model seems to better forecast the future Opening prices? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjonDYDWzRIv"
      },
      "source": [
        "print(\"RMSE (LSTM): %.2f\" % math.sqrt(metrics.mean_squared_error(real_stock_price, predicted_stock_price_LSTM))) #RMSE\n",
        "print(\"MAE (LSTM): %.2f\" % metrics.mean_absolute_error(real_stock_price, predicted_stock_price_LSTM)) #MAE\n",
        "\n",
        "print(\"RMSE (GRU): %.2f\" % math.sqrt(metrics.mean_squared_error(real_stock_price, predicted_stock_price_GRU))) #RMSE\n",
        "print(\"MAE (GRU): %.2f\" % metrics.mean_absolute_error(real_stock_price, predicted_stock_price_GRU)) #MAE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQg96m89zyVp"
      },
      "source": [
        "### Question 4\n",
        "Which model has a better predictive performance? which metric did you use? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRd1sHuPigjg"
      },
      "source": [
        "## Can we improve the model?\n",
        "\n",
        "There are several approaches we can try to potentially improve forecasting accuracy, including:\n",
        "\n",
        "* trying a deeper model with more layers and/or units in each layer\n",
        "* We can specify a longer time window for the network to learn from (for example 100 instead of 60). \n",
        "* Adjusting the Dropout rate. Pay attention to overfitting when using smaller dropout rates (the 20% we used is a common middle ground)\n",
        "\n",
        "We will try the first approach first"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC8uQ_2z4rRs"
      },
      "source": [
        "## Third model with 4 hidden layers\n",
        "\n",
        "Let's try a model with 4 LSTM layers (same number of units in each layer), and same Dropout rate after each layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1v0dXplipdA"
      },
      "source": [
        "# Initialisi#ng the RNN\n",
        "ts3= keras.Sequential()\n",
        "\n",
        "# Adding the first LSTM layer and some Dropout regularisation\n",
        "ts3.add(layers.LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], X_train.shape[2])))\n",
        "ts3.add(layers.Dropout(0.2))  #randomly drops 20% of observations to avoid overfitting\n",
        "\n",
        "# Adding a second LSTM layer and some Dropout regularisation\n",
        "ts3.add(layers.LSTM(units = 50, return_sequences = True))\n",
        "ts3.add(layers.Dropout(0.2))\n",
        "\n",
        "# Adding a third LSTM layer and some Dropout regularisation\n",
        "ts3.add(layers.LSTM(units = 50, return_sequences = True))\n",
        "ts3.add(layers.Dropout(0.2))\n",
        "\n",
        "# Adding a fourth LSTM layer and some Dropout regularisation\n",
        "ts3.add(layers.LSTM(units = 50))\n",
        "ts3.add(layers.Dropout(0.2))\n",
        "\n",
        "# Adding the output layer\n",
        "ts3.add(layers.Dense(units = 1))\n",
        "\n",
        "ts3.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xufM2RZY5WzC"
      },
      "source": [
        "# we use \"Mean Squared Error\" as our loss function \n",
        "ts3.compile(optimizer = 'adam', #can also try  optimizer='rmsprop'\n",
        "           loss = 'mean_squared_error', # can also try mean_absolute_error  \n",
        "           metrics=['MeanSquaredError', 'MeanAbsoluteError', 'MeanAbsolutePercentageError'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ui3j07dT5go-"
      },
      "source": [
        "# Fitting the model to the Training set\n",
        "history_3=ts3.fit(X_train, y_train, \n",
        "               epochs = 15, batch_size = 30,verbose=1,\n",
        "               validation_data=(X_test,y_test));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJgo5jhz5voz"
      },
      "source": [
        "### Evaluating the 3rd model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-h3rbfUb5g1U"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history_3.history['loss'],label=\"training\")\n",
        "plt.plot(history_3.history['val_loss'],label=\"validation\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtIYLkXq57hu"
      },
      "source": [
        "predicted_stock_price = ts3.predict(X_test)\n",
        "predicted_stock_price_3 = sc.inverse_transform(predicted_stock_price)\n",
        "\n",
        "# Visualising the results\n",
        "# you can add the plot for previous models' predictions by uncommenting the correspoding lines below\n",
        "plt.plot(real_stock_price, color = 'red', label = 'Real Stock Price')\n",
        "plt.plot(predicted_stock_price_3, color = 'blue', label = 'Predicted Stock Price (LSTM2)')\n",
        "#plt.plot(predicted_stock_price_GRU, color = 'yellow', label = 'Predicted Stock Price (GRU)')\n",
        "plt.plot(predicted_stock_price_LSTM, color = 'green', label = 'Predicted Stock Price (LSTM)')\n",
        "plt.title('Google Stock Price Prediction')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Google Stock Price')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GImaLY8G53mX"
      },
      "source": [
        "print(\"Root Mean squared error: %.2f\" % math.sqrt(metrics.mean_squared_error(real_stock_price, predicted_stock_price_3))) #RMSE\n",
        "print(\"Mean absolute error: %.2f\" % metrics.mean_absolute_error(real_stock_price, predicted_stock_price_3)) #MAE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWzeJPx07XBA"
      },
      "source": [
        "###Question 5\n",
        "Does the new model with 4 LSTM layers have a better performance? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1fTa4uA7zK8"
      },
      "source": [
        "By adding more layers we are building a more complex model, which can be helpful (to improve model performance) if the additional complexity helps capturing the underlying complexity in our data.In the current example, forecasting a univariate time series, this does not seem to be the case. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AANEDJND-4ly"
      },
      "source": [
        "## Changing window size (number of time steps)\n",
        "\n",
        "Let's try increasing the window size to 120 (number of time steps). This means we will use the opening price from the past 120 days to predict the value of today's opening price. \n",
        "\n",
        "To do that go to section **Specifying time window (time steps)** and change the value for *window* (first cell in the section) to **window=120** (from 60).\n",
        "Rerun all the subsequent cells in the **Data Preprocessing** section to reshape the data with the new time window. \n",
        "\n",
        "Then, rerun the first (LSTM network with 2 layers) and second (GRU network with 2 layers) model. *Run the following cell to keep the performance metrics for the two models (based on window=60) before changing the window size and retraining the models*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33x8bUPBBxio"
      },
      "source": [
        "print(\"RMSE (LSTM):%.2f\" % math.sqrt(metrics.mean_squared_error(real_stock_price, predicted_stock_price_LSTM)) +\";\",\n",
        "      \"MAE (LSTM):%.2f\" % metrics.mean_absolute_error(real_stock_price, predicted_stock_price_LSTM)) #MAE\n",
        "\n",
        "print(\"RMSE (GRU):%.2f\" % math.sqrt(metrics.mean_squared_error(real_stock_price, predicted_stock_price_GRU))+\";\",\n",
        "      \"MAE (GRU):%.2f\" % metrics.mean_absolute_error(real_stock_price, predicted_stock_price_GRU)) #MAE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knbT_gA2ESOu"
      },
      "source": [
        "###Question 6\n",
        "Does increasing the window size improve the predictive performance for any of the 2 models?\n",
        "\n",
        "Which one(s)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCkcbMedEifv"
      },
      "source": [
        "###Question 7 (Bonus question)\n",
        "Do you think reducing the window size (e.g., from 60 to 30) could improve model's forecasting performance (i.e., in terms of RMSE or MAE)? why? \n"
      ]
    }
  ]
}