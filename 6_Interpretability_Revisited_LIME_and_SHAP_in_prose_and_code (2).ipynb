{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmanr1VOVCDd"
      },
      "source": [
        "Adapted from the accompanying notebook of the article (https://blog.cloudera.com/ml-interpretability-lime-and-shap-in-prose-and-code/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adapted from the accompanying notebook of the article (https://blog.cloudera.com/ml-interpretability-lime-and-shap-in-prose-and-code/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVzgLHWUrgev"
      },
      "source": [
        "## Interpretability: LIME and SHAP in prose and code\n",
        "\n",
        " Model interpretability is an important step in the data science workflow. Being able to explain how a model works serves many purposes, including building trust in the model's output, satisfying regulatory requirements, model debugging, and verifying model safety, amongst other things. We have written a research report (access it free here) that discusses this topic in detail.\n",
        "In this article, we revisit two industry standard algorithms for interpretability - LIME and SHAP. We discuss how these two algorithms work, and show some code examples of how to implement them in python. At the end of this post, you should be familiar with:\n",
        "\n",
        "- An overview of model interpretability\n",
        "- Interpreting white box models, such as Linear/Logistic Regression (using model coefficients) and Tree models (using feature importance scores)\n",
        "- Interpreting black box models with LIME and SHAP (KernelExplainer, TreeExplainer) and how to implement this in python\n",
        "- Good practices for \"debugging\" LIME and SHAP explanations\n",
        "- Limitations of LIME/SHAP (a.k.a., when to choose LIME over SHAP)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rrTZ874kv7Hm"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "!pip install -q shap umap-learn lime\n",
        "!curl -L -O https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "import shap\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "import xgboost\n",
        "import seaborn as sns;\n",
        "import umap\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "\n",
        "idcol = 'customerID'\n",
        "labelcol = 'Churn'\n",
        "cols = (('gender', True),\n",
        "        ('SeniorCitizen', True),\n",
        "        ('Partner', True),\n",
        "        ('Dependents', True),\n",
        "        ('tenure', False),\n",
        "        ('PhoneService', True),\n",
        "        ('MultipleLines', True),\n",
        "        ('InternetService', True),\n",
        "        ('OnlineSecurity', True),\n",
        "        ('OnlineBackup', True),\n",
        "        ('DeviceProtection', True),\n",
        "        ('TechSupport', True),\n",
        "        ('StreamingTV', True),\n",
        "        ('StreamingMovies', True),\n",
        "        ('Contract', True),\n",
        "        ('PaperlessBilling', True),\n",
        "        ('PaymentMethod', True),\n",
        "        ('MonthlyCharges', False),\n",
        "        ('TotalCharges', False))\n",
        "\n",
        "def drop_non_features(df, cols):\n",
        "    return df[[c for c, _ in cols]]\n",
        "\n",
        "def drop_missing(df):\n",
        "    '''Remove rows with missing values'''\n",
        "    return df.replace(r'^\\s$', np.nan, regex=True).dropna()\n",
        "\n",
        "\n",
        "def clean(df):\n",
        "    # Make target variable a true boolean column\n",
        "    # Drop unpredictive column\n",
        "    df.drop(['customerID'], axis=1)\n",
        "\n",
        "def categorize(df, cols):\n",
        "    catcols = (c for c, iscat in cols if iscat)\n",
        "    for col in catcols:\n",
        "        df[col] = pd.Categorical(df[col])\n",
        "    return df\n",
        "\n",
        "def booleanize_senior_citizen(df):\n",
        "    '''Make SeniorCitizen 'Yes'/'No' like other columns in this dataset.'''\n",
        "    return df.replace({'SeniorCitizen': {1: 'Yes', 0: 'No'}})\n",
        "\n",
        "def splitdf(df, label):\n",
        "    return df.drop(label, axis=1), df[label]\n",
        "\n",
        "def load_dataset():\n",
        "    '''Return IBM customers and labels.'''\n",
        "    df = pd.read_csv(\"Telco-Customer-Churn.csv\")\n",
        "    df = drop_missing(df).reset_index()\n",
        "    df.index.name = 'id'\n",
        "    features, labels =  splitdf(df, labelcol)\n",
        "    features = booleanize_senior_citizen(features)\n",
        "    features = drop_non_features(features, cols)\n",
        "    features = categorize(features, cols)\n",
        "    labels = (labels == 'Yes')\n",
        "    return features, labels\n",
        "\n",
        "def setup_plot():\n",
        "  plt.rcParams[\"axes.grid.axis\"] =\"y\"\n",
        "  plt.rcParams[\"axes.grid\"] = True\n",
        "  plt.rcParams[\"legend.fontsize\"] = 14\n",
        "  plt.rc('grid', linestyle=\"dashed\", color='lightgrey', linewidth=1)\n",
        "  plt.rcParams[\"xtick.labelsize\"] = 15\n",
        "  plt.rcParams[\"ytick.labelsize\"]  = 15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGciO3fQYUZ9"
      },
      "source": [
        "## The Dataset (Customer Churn)\n",
        "\n",
        "For this post, we illustrate our discussion using a public dataset of 7,043 cable customers, around 25% of whom churned. There are 20 features for each customer, which are a mixture of intrinsic attributes of the person or home (gender, family size, etc.) and quantities that describe their service or activity (payment method, monthly charge, etc.).\n",
        "\n",
        "- The dataset contains both continuous and categorical features.\n",
        "\n",
        "- Depending on the type of model, we can make different decisions on how categorical fields are represented. For example, tree-based models can be trained directly with categorical features (label encoded), while other models (e.g. Logistic regression, Neural Networks) work better with one hot encoded categorical variables.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iD0m0mSvw2jI"
      },
      "outputs": [],
      "source": [
        "data, labels = load_dataset()\n",
        "\n",
        "# Get categorical column indexes\n",
        "cat_columns_ix_ = {c: i for i, c in enumerate(data.columns)\n",
        "                                if pd.api.types.is_categorical_dtype(data[c])}\n",
        "# categorical column names\n",
        "cat_columns  = [c for c in (data.columns) if pd.api.types.is_categorical_dtype(data[c])]\n",
        "# num_cat_columns  = [c for c in (data.columns) if pd.api.types.is_categorical_dtype(data[c])]\n",
        "\n",
        "# Make copies of data with various representations for categorical features.\n",
        "data_numeric = data.copy() # categorical fields are label encoded but numerical\n",
        "data_categorical = data.copy() # categorical fields are label encoded, numeric but categorical type\n",
        "data_ohe =  pd.get_dummies(data,columns=cat_columns, sparse=False) # categorical fields are one hot encoded\n",
        "\n",
        "# Convert categorical columns to numeric encoded labels\n",
        "for col in cat_columns:\n",
        "  data_numeric[col] = (LabelEncoder().fit_transform(data_numeric[col]))\n",
        "  data_categorical[col] = data_numeric[col].astype(\"category\")\n",
        "\n",
        "\n",
        "# Set data used for explorations\n",
        "# Hint .. Change this to other version of the data and observe changes in results.\n",
        "current_data = data_categorical\n",
        "print(\"Available features: \\n\", list(data_categorical.columns))\n",
        "print(\"Label Balance - [No Churn, Churn] : \", list(labels.value_counts()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWVn3qlnmq1r"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "We can train a set of models on the churn dataset and compare their performance. To simplify this process, we will use the sklearn library and utilize the `model.fit()` api to train the model while recording train and test accuracy. The following models are trained:\n",
        "\n",
        "- Naive Bayes\n",
        "- Logistic Regression\n",
        "- Decision Tree\n",
        "- Random Forest\n",
        "- Gradient Boosted Tree\n",
        "- Multilayer Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "k3msqVOHwatH"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "\n",
        "trained_models = [] #  keep track of all details for models we train\n",
        "def train_model(model, data, labels):\n",
        "  X = data\n",
        "  y = labels.values\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "  pipe = Pipeline([('scaler', StandardScaler()),('clf', model[\"clf\"])])\n",
        "  start_time = time.time()\n",
        "  pipe.fit(X_train, y_train)\n",
        "  train_time = time.time() - start_time\n",
        "\n",
        "  train_accuracy =  pipe.score(X_train, y_train)\n",
        "  test_accuracy = pipe.score(X_test, y_test)\n",
        "  model_details = {\"name\": model[\"name\"], \"train_accuracy\":train_accuracy, \"test_accuracy\":test_accuracy, \"train_time\": train_time, \"model\": pipe}\n",
        "  return model_details\n",
        "\n",
        "models = [\n",
        "          {\"name\": \"Naive Bayes\", \"clf\": GaussianNB()},\n",
        "          {\"name\": \"logistic regression\", \"clf\": LogisticRegressionCV()},\n",
        "          {\"name\": \"Decision Tree\", \"clf\": DecisionTreeClassifier()},\n",
        "          {\"name\": \"Random Forest\", \"clf\": RandomForestClassifier(n_estimators=100)},\n",
        "          {\"name\": \"Gradient Boosting\", \"clf\": GradientBoostingClassifier(n_estimators=100)}]\n",
        "\n",
        "for model in models:\n",
        "  model_details = train_model(model, current_data, labels)\n",
        "  trained_models.append(model_details)\n",
        "\n",
        "\n",
        "# visualize accuracy and run time\n",
        "setup_plot()\n",
        "model_df = pd.DataFrame(trained_models)\n",
        "model_df.sort_values(\"test_accuracy\", inplace=True)\n",
        "ax = model_df[[\"train_accuracy\",\"test_accuracy\", \"name\"]].plot(kind=\"bar\", x=\"name\", figsize=(19,5), title=\"Classifier Performance Sorted by Test Accuracy\")\n",
        "ax.legend([\"Train Accuracy\", \"Test Accuracy\"])\n",
        "for p in ax.patches:\n",
        "    ax.annotate( str( round(p.get_height(),3) ), (p.get_x() * 1.005, p.get_height() * 1.005))\n",
        "\n",
        "ax.title.set_size(20)\n",
        "plt.box(False)\n",
        "\n",
        "model_df.sort_values(\"train_time\", inplace=True)\n",
        "ax= model_df[[\"train_time\",\"name\"]].plot(kind=\"bar\", x=\"name\", figsize=(19,5), grid=True, title=\"Classifier Training Time (seconds)\")\n",
        "ax.title.set_size(20)\n",
        "ax.legend([\"Train Time\"])\n",
        "plt.box(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qI7CM3LlzoG"
      },
      "source": [
        "Figure shows a) Performance performance of 6 classifiers. While the (untuned) Decision tree and Random Forest models have high train accuracy, they don’t perform so well on the test set, indicating **overfitting** for this problem. Gradient boosted Tree and Logistic regression have a good balance of train/test accuracy for this dataset. b) Training time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTa-McMVu2UQ"
      },
      "source": [
        "## Explaining Models\n",
        "\n",
        "At this point, we have a few models trained, and we can use them to obtain predictions. Given the data for each cable customer, we can predict the probability that they will churn. However, what is not very clear is how each of these features contribute to the predicted churn probability. We can think of these explanations in **global** terms (i.e., how does each feature impact outcomes on the average for the entire datasheet?) or in **local** terms (i.e., how does each feature impact predictions for a given customer?).\n",
        "Some models have inbuilt properties that provide these sorts of explanations. These are typically referred to as **white box** models and examples include linear regression (model coefficients), logistic regression (model coefficients) and decision trees (feature importance). Due to their complexity, other models - such as Random Forests, Gradient Boosted Trees, SVMs, Neural Networks etc. - do not have straightforward methods for explaining their predictions. For these models, (also known as **black box** models), approaches such as LIME and SHAP can be applied.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m1ChyGcK2Qc"
      },
      "source": [
        "### Global Explanation - Logistic Regression Coefficients\n",
        "For models such as linear and logistic regression, we can look at the model coefficients to infer feature importance (note that coefficients need to be  [interpreted](https://web.ma.utexas.edu/users/mks/statmistakes/regressioncoeffs.html) with [care](https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/) for each model type).  This gives us some idea of how an increase/change in each feature might result in a change in the log odds that the customer will churn. We can also get a general understanding of how important a feature is for the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DG480JfS6F29"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,6))\n",
        "X_train, X_test, y_train, y_test = train_test_split(current_data, labels.values, random_state=42)\n",
        "logistic_reg_coeff = trained_models[1][\"model\"][\"clf\"].coef_\n",
        "color_list =  sns.color_palette(\"dark\", len(current_data.columns))\n",
        "top_x = 10\n",
        "logistic_reg_coeff = trained_models[1][\"model\"][\"clf\"].coef_[0]\n",
        "idx = np.argsort(np.abs(logistic_reg_coeff))[::-1]\n",
        "lreg_ax = plt.barh(current_data.columns[idx[:top_x]][::-1], logistic_reg_coeff[idx[:top_x]][::-1])\n",
        "for i,bar in enumerate(lreg_ax):\n",
        "  bar.set_color(color_list[idx[:top_x][::-1][i]])\n",
        "  plt.box(False)\n",
        "lr_title = plt.suptitle(\"Logistic Regression. Top \" + str(top_x) + \" Coefficients.\", fontsize=20, fontweight=\"normal\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3oa_OeHmXtN"
      },
      "source": [
        "# Explanations via Feature Importance Scores [Tree Based Models]\n",
        "\n",
        "Tree-based models have properties that let us infer the importance of a feature. For each decision tree, we can compute the mean decrease in impurity for each feature - i.e., how impactful the feature is in reducing the uncertainty (classifiers) or variance (regressors) of the decision tree prediction. This value is also known as the gini importance score. For each tree based model in our list of trained models, we can leverage the sklearn `feature_importance` implementation and visualize the average importance of each variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "G1Bub6sq41LT"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Create array of tree based models and plot feature importance scores\n",
        "tree_models = []\n",
        "setup_plot()\n",
        "color_list =  sns.color_palette(\"dark\", len(current_data.columns))\n",
        "top_x = 10 # number of x most important features to show\n",
        "for model in trained_models:\n",
        "  if hasattr(model[\"model\"][\"clf\"], 'feature_importances_'):\n",
        "    tree_models.append({\"name\":model[\"name\"], \"fi\": model[\"model\"][\"clf\"].feature_importances_})\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(1,3, figsize=(24, 8), facecolor='w', edgecolor='k')\n",
        "fig.subplots_adjust(hspace = 0.5, wspace=0.8)\n",
        "axs = axs.ravel()\n",
        "for i in range(len(tree_models)):\n",
        "  feature_importance = tree_models[i][\"fi\"]\n",
        "  indices = np.argsort(feature_importance)\n",
        "  indices = indices[-top_x:]\n",
        "\n",
        "  bars = axs[i].barh(range(len(indices)), feature_importance[indices], color='b', align='center')\n",
        "  axs[i].set_title( tree_models[i][\"name\"], fontweight=\"normal\", fontsize=16)\n",
        "\n",
        "  plt.sca(axs[i])\n",
        "  plt.yticks(range(len(indices)), [current_data.columns[j] for j in indices], fontweight=\"normal\", fontsize=16)\n",
        "\n",
        "  # print(len(plt.gca().get_yticklabels()), len(indices))\n",
        "  for i, ticklabel in enumerate(plt.gca().get_yticklabels()):\n",
        "    ticklabel.set_color(color_list[indices[i]])\n",
        "\n",
        "  for i,bar in enumerate(bars):\n",
        "    bar.set_color(color_list[indices[i]])\n",
        "  plt.box(False)\n",
        "\n",
        "plt.suptitle(\"Feature Importance for Tree Models. Top \" + str(top_x) + \" Features.\", fontsize=20, fontweight=\"normal\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1JhdmIxSHo5"
      },
      "source": [
        "These feature importance values look interesting. All 3 models appear to consistently rank `Monthly Charges`, `Total Charges`, `Contract` and `Tenure` in the top 5 salient features, suggesting they are important features.\n",
        "\n",
        "There are several limitations here.\n",
        "- The feature importance scores are relative, making it hard to interpret with respect to the predicted outcome. While they tell us that `Total Charges` is relatively more important than Contract for the Decision Tree model, they do not tell us how much a 1$ USD increase in `Total Charges` impacts the probability of a customer churning.\n",
        "- Feature importance measures are global estimates across the entire training dataset. For a subset of customers, within the same model, the order and magnitude of feature importance may change (see the section below on local instance explanations). Thus, while we know `Total Charges` are important on the average we cannot confidently answer the question:  how does Total Charges affect a specific customer?\n",
        "\n",
        "- There are known biases related to how the feature importance score is computed. See ([Bias in random forest variable importance measures: Illustrations, sources and a solution](https://link.springer.com/article/10.1186%2F1471-2105-8-25))\n",
        "\n",
        "\n",
        "These limitations make it desirable to explore local explanations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiWukyKgu6_q"
      },
      "source": [
        "## Explanations with LIME\n",
        "\n",
        "Local Interpretable Model-agnostic Explanation ([LIME](https://arxiv.org/abs/1602.04938)) provides a fast and relatively simple method for `locally` explaining black box models. The LIME algorithm can be simplified into a few steps\n",
        "\n",
        "- For a given data point, randomly perturb its features repeatedly. For tabular data, this entails adding a small amount of noise to each feature.  \n",
        "- Get predictions for each perturbed data instance. This helps us build up a local picture of the decision surface at that point.\n",
        "- Use predictions to compute an approximate linear \"explanation model\" using predictions. Coefficients of the linear model are used as explanations.\n",
        "\n",
        "\n",
        "The LIME python library provides interfaces for explaining models built on tabular (TabularExplainer), image (LimeImageExplainer), and text data (LimeTextExplainer).\n",
        "\n",
        "In the following section, we will attempt to explain predictions from a single test data instance for all our trained models using the LimeTabularExplainer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qh51qZDGq_-"
      },
      "source": [
        "## LIME Tabular Explainer: Explain a test data instance for all models\n",
        "\n",
        "In the following section, we will generate and visualize lime explanations for a given data point in our test set. We will do this for all our trained models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiyjB5RiTSqN"
      },
      "outputs": [],
      "source": [
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "X_train, X_test, y_train, y_test = train_test_split(current_data, labels, random_state=42)\n",
        "\n",
        "def get_lime_explainer(model, data, labels):\n",
        "\n",
        "  cat_feat_ix = [i for i,c in enumerate(data.columns) if pd.api.types.is_categorical_dtype(data[c])]\n",
        "  feat_names = list(data.columns)\n",
        "  class_names = list(labels.unique())\n",
        "  scaler = model[\"model\"][\"scaler\"]\n",
        "  data = scaler.transform(data) # scale data to reflect train time scaling\n",
        "  lime_explainer = LimeTabularExplainer(data,\n",
        "                                      feature_names=feat_names,\n",
        "                                      class_names=class_names,\n",
        "                                      categorical_features=cat_feat_ix ,\n",
        "                                      mode=\"classification\"\n",
        "                                      )\n",
        "  return lime_explainer\n",
        "\n",
        "def lime_explain(explainer, data, predict_method, num_features):\n",
        "  explanation = explainer.explain_instance(data, predict_method, num_features=num_features)\n",
        "  return explanation\n",
        "\n",
        "lime_data_explainations = []\n",
        "lime_metrics = []\n",
        "lime_explanation_time = []\n",
        "feat_names = list(current_data.columns)\n",
        "test_data_index = 6\n",
        "for current_model in trained_models:\n",
        "  scaler = current_model[\"model\"][\"scaler\"]\n",
        "  scaled_test_data = scaler.transform(X_test)\n",
        "  predict_method = current_model[\"model\"][\"clf\"].predict_proba\n",
        "\n",
        "  start_time = time.time()\n",
        "  # explain first sample from test data\n",
        "  lime_explainer = get_lime_explainer(current_model, X_train, y_train)\n",
        "  explanation = lime_explain(lime_explainer, scaled_test_data[test_data_index], predict_method, top_x)\n",
        "  elapsed_time = time.time() - start_time\n",
        "\n",
        "  ex_holder = {}\n",
        "  for feat_index,ex in explanation.as_map()[1] :\n",
        "    ex_holder[feat_names[feat_index]] = ex\n",
        "\n",
        "  lime_data_explainations.append(ex_holder)\n",
        "  actual_pred = predict_method(scaled_test_data[test_data_index].reshape(1,-1))\n",
        "  perc_pred_diff =  abs(actual_pred[0][1] - explanation.local_pred[0])\n",
        "  lime_explanation_time.append({\"time\": elapsed_time, \"model\": current_model[\"name\"] })\n",
        "  lime_metrics.append({\"lime class1\": explanation.local_pred[0], \"actual class1\": actual_pred[0][1], \"class_diff\": round(perc_pred_diff,3), \"model\": current_model[\"name\"] })\n",
        "  # break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mC47gHuw_SBb"
      },
      "outputs": [],
      "source": [
        "def plot_lime_exp(fig, fig_index, exp_data, title):\n",
        "  features =  list(exp_data.keys())[::-1]\n",
        "  explanations = list(exp_data.values())[::-1]\n",
        "  ax = fig.add_subplot(int(fig_index))\n",
        "  lime_bar = ax.barh( features, explanations )\n",
        "  ax.set_title(title, fontsize = 20)\n",
        "  for i,bar in enumerate(lime_bar):\n",
        "    bar.set_color(color_list[list(current_data.columns).index(features[i])])\n",
        "    plt.box(False)\n",
        "\n",
        "fig = plt.figure(figsize=(19,8))\n",
        "\n",
        "# Plot lime explanations for trained models\n",
        "for i, dex in enumerate(lime_data_explainations):\n",
        "  fig_index = \"23\" + str(i+1)\n",
        "  plot_lime_exp(fig, fig_index, lime_data_explainations[i], trained_models[i][\"name\"])\n",
        "\n",
        "plt.suptitle( \" LIME Explanation for single test data instance.  Top \" + str(top_x) + \" Features\", fontsize=20, fontweight=\"normal\")\n",
        "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "\n",
        "# Plot run time for explanations\n",
        "lx_df = pd.DataFrame(lime_explanation_time)\n",
        "lx_df.sort_values(\"time\", inplace=True)\n",
        "setup_plot()\n",
        "lx_ax = lx_df.plot(kind=\"bar\", x=\"model\", title=\"Runtime (seconds) for single test data instance LIME explanation\", figsize=(22,6))\n",
        "lx_ax.title.set_size(20)\n",
        "lx_ax.legend([\"Run time\"])\n",
        "plt.box(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pENQPAkob_yL"
      },
      "source": [
        "## Debugging  LIME: Should I trust the Explanation?\n",
        "\n",
        "\n",
        "Underneath, the LIME algorithm uses an approximate linear model to derive local explanations. Like any other ML model, this explanation model can have *issues* too. So, what can we do to build   confidence in the quality of an explanation. As a first step, we can check if the local model is indeed a good approximator for the original model.    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPdqe0WAcGRd"
      },
      "outputs": [],
      "source": [
        "# Plot run time for explanations\n",
        "lime_metrics_df = pd.DataFrame(lime_metrics)\n",
        "lime_metrics_df_ax = lime_metrics_df[[\"lime class1\", \"actual class1\", \"model\"]].plot(kind=\"bar\", x=\"model\", title=\"LIME Actual Prediction vs Local Prediction \", figsize=(22,6))\n",
        "lime_metrics_df_ax.title.set_size(20)\n",
        "lime_metrics_df_ax.legend([\"Lime Local Prediction\", \"Actual Prediction\"])\n",
        "plt.box(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1SRBtGjI5mZ"
      },
      "source": [
        "The plot above shows the predictions made by the LIME local model and the original model for the explained data instance. Both numbers should be close.  When they are not, this may raise questions as to if we can trust the explanation. There are few things that can be done:\n",
        "-  Modify the parameters of LIME to yield a better explanation. E.g. increase the number of perturbations (LIME ) or kernel width (see related discussion  [here](https://github.com/marcotcr/lime/issues/209)),\n",
        "- Improve our original model (In this case, we know that the Decision Tree shows signs of overfitting).\n",
        "\n",
        "The next explanation method we will consider (SHAP) aims to address such inconsistencies. Let's go!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhzDJbBLz-Yt"
      },
      "source": [
        "## Explanations with SHAP\n",
        "\n",
        "To provide some intuition on how SHAP works, consider the following scenario. We have a group of data scientists (Sarah, Jessica, and Patrick) who collaborate to build a great predictive model for their company. At the end of the year, their efforts result in an increase in profit of which $5 million must now be shared amongst our 3 heroes. Assuming we have good ways to measure (or simulate) the contributions of each data scientist, how can we allocate this profit such that each person is fairly rewarded commensurate to their actual contribution?\n",
        "\n",
        "\n",
        "[Shapley values](https://en.wikipedia.org/wiki/Shapley_value) provide a method for this specific type of allocation (collaborative multiplayer game setting) with a set of desirable axiomatic properties (Efficiency, Symmetry, Linearity, Anonymity, Marginalism) that guarantee fairness. These values are computed by computing the average marginal contribution of each person across all possible orderings. For example, imagine we assign only Sarah to the project and note the increase in profit (their marginal contribution). We then add Jessica and note the corresponding increase. We then add Patrick and note their contribution. This is repeated for all possible orderings (e.g. {Patrick, Jessica, Sarah}, {Jessica, Sarah, Patrick}, etc ) and the average marginal contribution for each person is computed.\n",
        "Extending this to machine learning, we can think of each feature as comparable to our data scientists and the model prediction as the profits. To explain our model, we repeatedly add each feature and note its marginal contribution to model prediction. Importantly, we want to use the Shapley values to assign credit to each feature, because they provide two important guarantees (e.g., LIME, Feature Permutation, Feature Importance) that other methods do not provide:\n",
        "\n",
        "\n",
        "- local accuracy (an approximate model used to explain the original model should match the output of the original model for a given input)\n",
        "- consistency (if the original model changes such that a feature has a larger impact in every possible ordering, then its attribution should not decrease)\n",
        "\n",
        "\n",
        "\n",
        "In practice, a few simplifications are required to compute Shapley values. Perhaps the most important is related to how we simulate the adding or removal of features while computing model prediction. This is challenging because there is no straightforward way to \"remove\" a feature for most predictive models at test time. We can either replace the feature with its mean value, median value. In the SHAP library implementation, a “missing” feature is simulated by replacing the feature with the values it takes in the background dataset.\n",
        "\n",
        "\n",
        "## The SHAP Library Implementation.\n",
        "\n",
        "The SHAP library contains implementations for several types of explanations that leverage Shapley values. These include the `TreeExplainer` which is optimized (and fast) for tree based models; `DeepExplainer` and `GradientExplainer` for neural networks; and  `KernelExplainer`, which makes no assumptions about the underlying model to be explained (model agnostic like LIME).\n",
        "\n",
        "To explain a model on a test set using `KernelExplainer`, the SHAP library api is as follows:\n",
        "\n",
        "```\n",
        "import shap\n",
        "explainer = shap.KernelExplainer(model.predict_proba, background_data)\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "```\n",
        "\n",
        "To unpack and understand the results from SHAP KernelExplainer, there are a few terms worth clarifying.\n",
        "\n",
        "\n",
        "| Variable \t| Description \t|\n",
        "|--------------------------\t|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\t|\n",
        "| model \t| the model to be explained \t|\n",
        "| background_data \t| This is a required argument to KernelExplainer. Since most models aren’t designed to handle arbitrary missing data at test time, SHAP simulates a “missing” feature by replacing it with the values it takes in the background dataset. For small problems, this background dataset can be the whole training set, but for larger problems, it is suggested that a subsample of the training set (or the kmeans function to summarize the dataset) is used. Background data is optional for tree-based models. \t|\n",
        "| explainer.expected_value \t| This is a field in the explainer object is displayed as the baseline in a SHAP force plot. It should be the same as the mean of the model output over the background dataset. One simple task which I found to be useful is to manually compute the mean prediction on the background dataset and see how it corresponds to the expected value output by SHAP. \t|\n",
        "| shap_values \t| The shap_values returned by the explainer object are a measure of how each feature contributes to the difference between the model’s expected value and the prediction for that instance. The units of the Shapley values are in the units of the target variable. The sum of the shap values should be equal to the difference between the base value and the model prediction. \t|\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9D6B4Xu172Q"
      },
      "source": [
        "## Kernel Explainer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1yxTyWS2JGc"
      },
      "outputs": [],
      "source": [
        "current_model = trained_models[3] # Explain the Random Forest Model\n",
        "clf = current_model[\"model\"][\"clf\"]\n",
        "scaler = current_model[\"model\"][\"scaler\"]\n",
        "scaled_train_data = scaler.transform(X_train)\n",
        "sub_sampled_train_data = shap.sample(scaled_train_data, 600, random_state=0) # use 600 samples of train data as background data\n",
        "\n",
        "scaled_test_data = scaler.transform(X_test)\n",
        "subsampled_test_data =scaled_test_data[test_data_index].reshape(1,-1)\n",
        "\n",
        "start_time = time.time()\n",
        "explainer = shap.KernelExplainer(clf.predict_proba, sub_sampled_train_data)\n",
        "shap_values = explainer.shap_values(subsampled_test_data,  l1_reg=\"aic\")\n",
        "elapsed_time = time.time() - start_time\n",
        "# explain first sample from test data\n",
        "print(\"Kernel Explainer SHAP run time\", round(elapsed_time,3) , \" seconds. \", current_model[\"name\"])\n",
        "print(\"SHAP expected value\", explainer.expected_value)\n",
        "print(\"Model mean value\", clf.predict_proba(scaled_train_data).mean(axis=0))\n",
        "print(\"Model prediction for test data\", clf.predict_proba(subsampled_test_data))\n",
        "shap.initjs()\n",
        "pred_ind = 0\n",
        "shap.force_plot(explainer.expected_value[1], shap_values[1][0], subsampled_test_data[0], feature_names=X_train.columns)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFzyPAn5veDn"
      },
      "outputs": [],
      "source": [
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, subsampled_test_data, feature_names=X_train.columns, max_display=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITRgkIa2vEs4"
      },
      "source": [
        "## Tree Explainer\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrbBuGLRzci8"
      },
      "source": [
        "TreeExplainer on random forests takes about 0.091 seconds. This is compared to > 12.15 seconds required by KernelExplainer ( > **100x** faster)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCOJFGMj3fq5"
      },
      "outputs": [],
      "source": [
        "current_model = trained_models[3]\n",
        "clf = current_model[\"model\"][\"clf\"]\n",
        "scaler = current_model[\"model\"][\"scaler\"]\n",
        "\n",
        "scaled_test_data = scaler.transform(X_test)\n",
        "subsampled_test_data =scaled_test_data[test_data_index].reshape(1,-1)\n",
        "\n",
        "# explain first sample from test data\n",
        "start_time = time.time()\n",
        "explainer = shap.TreeExplainer(clf)\n",
        "shap_values = explainer.shap_values(subsampled_test_data)\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "print(\"Tree Explainer SHAP run time\", round(elapsed_time,3) , \" seconds. \", current_model[\"name\"])\n",
        "print(\"SHAP expected value\", explainer.expected_value)\n",
        "print(\"Model mean value\", clf.predict_proba(scaled_train_data).mean(axis=0))\n",
        "print(\"Model prediction for test data\", clf.predict_proba(subsampled_test_data))\n",
        "shap.initjs()\n",
        "pred_ind = 0\n",
        "shap.force_plot(explainer.expected_value[1], shap_values[1][0], subsampled_test_data[0], feature_names=X_train.columns)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMnMyckAwsdk"
      },
      "source": [
        "## Kernel Explainer - Explain a test data instance for all models\n",
        "\n",
        "In the following section, we will generate and visualize explanations using the SHAP KernelExplainer for a given data point in our test set (same as we did for LIME)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6eFEh-oOwvL"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "\n",
        "def get_kernel_shap_explainer(model, background_data, train_data):\n",
        "  shap_explainer = shap.KernelExplainer(model.predict_proba, background_data)\n",
        "  return shap_explainer\n",
        "\n",
        "def shap_explain(explainer, test_data):\n",
        "  shap_values = explainer.shap_values(test_data, l1_reg=\"aic\")\n",
        "\n",
        "  return shap_values\n",
        "\n",
        "shap_data_explainations = []\n",
        "shape_explanation_time = []\n",
        "feat_names = list(current_data.columns)\n",
        "data_subsample = 500\n",
        "for current_model in trained_models:\n",
        "  scaler = current_model[\"model\"][\"scaler\"]\n",
        "  scaled_test_data = scaler.transform(X_test)\n",
        "  scaled_train_data = scaler.transform(X_train)\n",
        "  sampled_scaled_train_data = shap.sample(scaled_train_data, data_subsample) # subsample background data to make things faster\n",
        "\n",
        "  start_time = time.time()\n",
        "  shap_explainer  = get_kernel_shap_explainer(current_model[\"model\"][\"clf\"], sampled_scaled_train_data, scaled_train_data)\n",
        "\n",
        "  # explain first sample from test data\n",
        "  sampled_scaled_test_data = scaled_test_data[test_data_index].reshape(1,-1)\n",
        "  shap_values = shap_explain(shap_explainer, sampled_scaled_test_data)\n",
        "  elapsed_time = time.time() - start_time\n",
        "  idx = np.argsort(np.abs(shap_values[1][0]))[::-1]\n",
        "  ex_holder = { feat_names[idx[i]] : shap_values[1][0][idx[i]] for i in range(top_x)}\n",
        "\n",
        "\n",
        "  shap_data_explainations.append(ex_holder)\n",
        "  shape_explanation_time.append({\"time\": elapsed_time, \"model\": current_model[\"name\"] })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YRBzUfRVTTD"
      },
      "outputs": [],
      "source": [
        "def plot_shap_exp(fig, fig_index, exp_data, title):\n",
        "  features =  list(exp_data.keys())[::-1]\n",
        "  explanations = list(exp_data.values())[::-1]\n",
        "  ax = fig.add_subplot(int(fig_index))\n",
        "  lime_bar = ax.barh( features, explanations )\n",
        "  ax.set_title(title, fontsize = 20)\n",
        "  for i,bar in enumerate(lime_bar):\n",
        "    bar.set_color(color_list[list(current_data.columns).index(features[i])])\n",
        "    plt.box(False)\n",
        "\n",
        "\n",
        "# Plot SHAP explanations for a given test set item\n",
        "fig = plt.figure(figsize=(19,8))\n",
        "for i, dex in enumerate(shap_data_explainations):\n",
        "  fig_index = \"23\" + str(i+1)\n",
        "  plot_lime_exp(fig, fig_index, shap_data_explainations[i], trained_models[i][\"name\"])\n",
        "\n",
        "plt.suptitle( \"Kernel SHAP Explanation for single test data instance.  Top \" + str(top_x) + \" Features\", fontsize=20, fontweight=\"normal\")\n",
        "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "\n",
        "# Plot SHAP explanation run time\n",
        "shapx_df = pd.DataFrame(shape_explanation_time)\n",
        "shapx_df.sort_values(\"time\", inplace=True)\n",
        "# setup_plot()\n",
        "# shapx_df_ax = shapx_df.plot(kind=\"line\", x=\"model\", title=\"Runtime (seconds) for single instance SHAP explanation\", figsize=(22,6))\n",
        "# shapx_df_ax.title.set_size(20)\n",
        "# shapx_df_ax.legend([\"Run time\"])\n",
        "# plt.box(False)\n",
        "\n",
        "\n",
        "# Plot both LIME and SHAP explanation run times\n",
        "m_df =  shapx_df.merge(lx_df, on=\"model\", suffixes=(\"_SHAP\", \"_LIME\"))\n",
        "m_df.head()\n",
        "mx_df_ax = m_df.plot(kind=\"bar\", x=\"model\", title=\"Kernel SHAP vs LIME: Runtime (seconds) for single instance explanation\", figsize=(22,6))\n",
        "mx_df_ax.title.set_size(20)\n",
        "mx_df_ax.legend([\"Run time for SHAP\", \"Run time for LIME\"])\n",
        "plt.box(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HC532mLR88Fx"
      },
      "source": [
        "## Debugging SHAP KernelExplainer: Small background Dataset may lead to inconsistent expected value.\n",
        "\n",
        "The process used by SHAP to simulate missing values draws from a background dataset (usually the training set). When the training dataset is large, the authors of SHAP suggest the use of a subsample of the training set as background data to reduce computation costs.  A side effect of using a small sub sample is that the expected value used by SHAP may differ from the model's mean value. The plots below compare the SHAP expected value and model mean prediction on training set for multiple background sample sizes.\n",
        "\n",
        "- Run time increases (linearly) with increase in size of background dataset\n",
        "- Expected value is closer to mean value on enter train set as the size of background subsample increases. (ideally these numbers should be the same. See relevant  [discussion on Github](https://github.com/slundberg/shap/issues/352))\n",
        "\n",
        "The above results raise some questions as to the quality of explanations when a subsample is used as background data for SHAP. It is important to note that the TreeExplainer sidesteps this issue as it does not require a background dataset. Per the SHAP documentation\n",
        "\n",
        "```\n",
        "This argument (background data) is optional when feature_perturbation=”tree_path_dependent”, since in that case we can use the number of training samples that went down each tree path as our background dataset (this is recorded in the model object).\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxwXV-uNzbaR"
      },
      "outputs": [],
      "source": [
        "current_model = trained_models[3]\n",
        "clf = current_model[\"model\"][\"clf\"]\n",
        "scaler = current_model[\"model\"][\"scaler\"]\n",
        "\n",
        "sample_sizes = [50, 100, 150, 200, 300, 600, 1000, 2000, 3000]\n",
        "metric_holder = []\n",
        "\n",
        "\n",
        "for sample_size in sample_sizes:\n",
        "  scaled_train_data = scaler.transform(X_train)\n",
        "  sub_sampled_train_data = shap.sample(scaled_train_data, sample_size, random_state=0) # use x samples of train data as background data\n",
        "\n",
        "  scaled_test_data = scaler.transform(X_test)\n",
        "  test_data_index = 10\n",
        "  subsampled_test_data =scaled_test_data[test_data_index].reshape(1,-1)\n",
        "\n",
        "  start_time = time.time()\n",
        "  explainer = shap.KernelExplainer(clf.predict_proba, sub_sampled_train_data)\n",
        "  shap_values = explainer.shap_values(subsampled_test_data,  l1_reg=\"aic\")\n",
        "  elapsed_time = time.time() - start_time\n",
        "\n",
        "  metric_holder.append({\"class0 exp\": explainer.expected_value[0],\n",
        "                        \"class1 exp\":explainer.expected_value[1],\n",
        "                        \"run time\": elapsed_time,\n",
        "                        \"sample size\": sample_size\n",
        "                        })\n",
        "\n",
        "metric_df = pd.DataFrame(metric_holder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNDj6bZB_AjF"
      },
      "outputs": [],
      "source": [
        "mean_pred = clf.predict_proba(scaled_train_data).mean(axis=0)\n",
        "metric_df[\"mean prediction class0\"] = mean_pred[0]\n",
        "metric_df[\"mean prediction class1\"] = mean_pred[1]\n",
        "metric_df.head()\n",
        "setup_plot()\n",
        "metrix_ax = metric_df[[\"sample size\", \"run time\"]].plot(kind=\"line\", x=\"sample size\", title=\"Kernel SHAP Run time (seconds) vs Background Data Size\", figsize=(22,4))\n",
        "metrix_ax.title.set_size(20)\n",
        "plt.box(False)\n",
        "\n",
        "setup_plot()\n",
        "metrix_ax = metric_df[[\"sample size\", \"mean prediction class0\", \"class0 exp\"]].plot(kind=\"line\", x=\"sample size\", title=\"Expected Value vs Mean Prediction\", figsize=(22,4))\n",
        "metrix_ax.title.set_size(20)\n",
        "metrix_ax.legend([\"Mean prediction on train set\", \"SHAP Expected Value\"])\n",
        "plt.box(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vErJda7XcjqD"
      },
      "source": [
        "## LIME vs SHAP : When to use what?\n",
        "\n",
        "LIME and SHAP are both good methods for explaining models. In theory, SHAP is the better approach as it provides mathematical guarantees for the accuracy and consistency of explanations. In practice, the model agnostic implementation of SHAP (KernelExplainer) is slow, even with approximations. This speed issue is of much less concern if you are using a tree based model and can take advantage of the optimizations implemented in SHAP TreeExplainer (we saw it could be up to 100x faster than KernelExplainer).\n",
        "\n",
        "Some additional limitations of both methods are mentioned below\n",
        "\n",
        "- LIME is not designed to work with one hot encoded data. Considering that each data point is perturbed to create the approximate model, perturbing a one hot encoded variable may result in unexpected (meaningless) features. See discussion [here](https://github.com/marcotcr/lime/issues/153)\n",
        "- LIME depends on the ability to perturb samples in meaningful ways. This perturbation is use case specific. E.g., for tabular data, this entails adding random noise to each feature; for images, this entails replacing superpixels within the image with some mean value or zeroes; for text, this entails removing words from the text. It is often useful to think through any side effects of these perturbation strategies with respect to your data to further build trust in the explanation.\n",
        "- In some cases, the local model built by LIME may fail to approximate the behaviour of the original model. It is good practice to check for such inconsistencies before trusting LIME explanations.\n",
        "\n",
        "- LIME works with models that output probabilities for classification problems. Models like SVMs are not particularly designed to output probabilities (though they can be coerced into this with some [issues](https://scikit-learn.org/stable/modules/svm.html).). This may introduce some bias into the explanations.\n",
        "\n",
        "- SHAP depends on background datasets to infer a baseline/expected value. For large datasets, it is computationally expensive to use the entire dataset and we have to rely on appromixations (e.g. subsample the data). This has implications for the accuracy of the explanation.\n",
        "\n",
        "- SHAP explains the deviation of a prediction from the expected/baseline value which is estimated using the training dataset. Depending on the specific use case, it may be more meaningful to compute the expected value using a specific subset of the training set as opposed to the entire training set. For example, it may be more meaningful to explain a churn prediction with respect to how it deviates from customers who did not churn. Here, we might want to use the dataset of customers who did not churn as our background dataset.  See this issue [here](https://github.com/slundberg/shap/issues/435)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZQ3I4GuPwvw"
      },
      "source": [
        "## Conclusion and Further Reading\n",
        "\n",
        "In this notebook, we have looked at LIME and SHAP. Actually, we have looked at just subsets of both of these remarkable tools, focusing on explaining models built on tabular data.  It is worth noting that  these methods can be extended to work on text, and image data as well. For additional reading on other interpretability methods, the [Interpretable ML book by  Christoph Molnar  is recommended](https://christophm.github.io/interpretable-ml-book/).\n",
        "\n",
        "- Interpretability - Cloudera Fast Forward Labs https://ff06-2020.fastforwardlabs.com/\n",
        "- SHAP vs Integrated Gradients https://blog.fiddler.ai/2019/08/should-you-explain-your-predictions-with-shap-or-ig/\n",
        "- Interpretable Machine Learning https://christophm.github.io/interpretable-ml-book/\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
